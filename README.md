# AI Agent Memory Cache

**Repository:** [https://github.com/JoshW-dev/ai-agent-memory-cache.git](https://github.com/JoshW-dev/ai-agent-memory-cache.git)

This project implements an intelligent AI agent with two complementary learning and caching mechanisms:
1.  A **`CapturingAgent`** that dynamically selects or creates tools to handle user prompts, learning from feedback to improve its tool choices over time.
2.  A **`MemoryCache`** that stores successful sequences of actions generated by the agent, allowing semantically similar requests to be fulfilled instantly by replaying these proven sequences.

## System Diagrams

### Component Architecture
![Memory Cache Component Diagram](./diagrams/component_diagram.svg)

### Data Flow
![Memory Cache Data Flow Diagram](./diagrams/data_flow_diagram.svg)

## 1. Core Concepts

At its heart, the system aims to make an AI agent more efficient and adaptable:

*   **Efficiency through Caching:** Storing successful multi-step plans (`ActionSequence`s) in the `MemoryCache` avoids costly re-planning by an LLM for repeated or similar requests.
*   **Adaptability through Learning:**
    *   **Agent-Level Learning (Tool Choice):** The `CapturingAgent` learns which tools are best for which types of prompts through semantic similarity. When a user upvotes a tool choice, the agent stores the prompt's embedding with that tool, making it more likely to pick that tool for semantically similar future prompts (even if the wording is different).
    *   **Cache-Level Learning (Sequence Viability):** The `MemoryCache` learns which stored `ActionSequence`s are reliable. User feedback (upvotes/downvotes on a cached sequence's outcome) adjusts a score for that sequence. Low-scoring sequences are eventually evicted, ensuring the cache remains filled with effective solutions.
    *   **Dynamic Tool Creation:** If the `CapturingAgent` doesn't have a suitable existing tool (similarity below threshold of 0.4), it can use an LLM to define a new one on the fly, expanding its capabilities based on user needs. The agent prevents duplicate tools by checking if the suggested name already exists.

## 2. How It Works: A Two-Layered Approach

When a user submits a prompt:

1.  **`MemoryCache` Lookup:** The system first checks the `MemoryCache`.
    *   It converts the user's prompt into an embedding (a numerical representation of its meaning).
    *   It queries ChromaDB (the vector store for the cache) to find previously stored `ActionSequence`s whose original prompts are semantically similar to the current user prompt.
    *   **Cache Hit:** If a highly similar prompt is found with a good historical score, the `MemoryCache` returns its stored `ActionSequence`. This sequence is then presented to the user, bypassing the agent entirely for this turn.
    *   **Cache Miss:** If no suitable `ActionSequence` is found in the cache, the system proceeds to the `CapturingAgent`.

2.  **`CapturingAgent` Processing (on Cache Miss):**
    *   **Tool Selection/Creation:** The `CapturingAgent` determines the best way to handle the prompt:
        *   It compares the prompt's embedding against the embeddings of its available tools (each tool's representation includes its core description and embeddings from previously upvoted prompts for that tool).
        *   If a known tool is a good semantic match (above `TOOL_SIMILARITY_THRESHOLD` of 0.4), it's selected. For example, a prompt like "Set player speed to 20" might match with `SetPlayerAttribute` or even `SetPlayerHealth` if they're semantically similar enough.
        *   If no existing tool is a good match, the agent attempts to define a new tool using the LLM. For example, a prompt like "Set player's health to 75" might create a new `SetPlayerHealth` tool if `SetPlayerAttribute` is below the similarity threshold.
        *   If the LLM suggests a tool name that already exists, the agent prevents duplicate creation and either selects that existing tool or creates a tool with a different name.
        *   If no tool can be selected or created, the agent falls back to generating a direct answer using the LLM.
    *   **Tool Input Generation & Execution:** If a tool is selected, the LLM generates the appropriate input for that tool based on the user prompt. The tool is then executed.
    *   **History Generation:** The agent records its actions (tool chosen, input, observation, similarity scores, or direct answer) as an `ActionSequence`.
    *   This newly generated `ActionSequence` is then typically stored in the `MemoryCache` with an initial high score.

3.  **User Feedback Integration:**
    *   **Feedback on Cached Sequence:** If an `ActionSequence` was retrieved from the cache, the user can indicate if it was helpful. This feedback updates the score of that `ActionSequence` in the `MemoryCache`. Very low scores lead to eviction.
    *   **Feedback on Agent's Tool Choice:** If the `CapturingAgent` ran (on a cache miss), the user can provide feedback on the tool it chose (or the direct answer it gave). This is primarily managed by the application layer (`app.py` or `mock_agent_demo.py`):
        *   **Upvote:** The `CapturingAgent`'s `record_tool_usage_feedback` method is called. The embedding of the user prompt is added to the chosen tool's `additional_prompt_embeddings`. This makes the tool more likely to be chosen for similar prompts in the future.
        *   **Downvote:** The `CapturingAgent` is asked to `run` again for the *same user prompt*, but with the downvoted tool added to an `exclude_tool_names` list. This forces the agent to try a different existing tool or attempt to create another new tool.

## 3. Key Features

*   **`MemoryCache` for Action Sequences:**
    *   Stores `(prompt, ActionSequence)` pairs.
    *   Uses OpenAI embeddings for prompts and ChromaDB for vector storage.
    *   Performs similarity-based lookups for cached sequences.
    *   Updates sequence scores via EMA based on feedback; evicts low-scoring sequences.
*   **`CapturingAgent` for Intelligent Task Execution:**
    *   Selects tools based on semantic similarity (tool description + learned prompt embeddings).
    *   Dynamically creates new tools if existing ones aren't suitable (with safeguards against duplicate names).
    *   Generates tool inputs and direct answers using an LLM.
    *   Learns from upvoted tool choices by augmenting tool embeddings.
    *   Handles downvoted tool choices by attempting alternatives on retry.
    *   Produces a detailed `ActionSequence` log of its operations.
*   **Customizable Tools:** Define specific capabilities for the agent.
*   **Interactive Demos:** `mock_agent_demo.py` (CLI) and `app.py` (Streamlit UI) for testing and interaction.

## 4. Project Structure

```
├── README.md             # This file
├── development_plan.md   # Tracks tasks and progress
├── diagrams/             # SVG diagrams
│   ├── component_diagram.svg
│   └── data_flow_diagram.svg
├── llm_module/           # Core agent logic
│   ├── __init__.py
│   ├── agent.py          # Base Agent class
│   ├── capturing_agent.py # The intelligent agent with learning
│   ├── custom_tools.py   # Example tool definitions
│   ├── llm.py            # OpenAI API wrapper
│   └── tools/
│       ├── __init__.py
│       └── base.py       # Base Tool class (stores embeddings)
├── memory_cache.py       # MemoryCache class for ActionSequences
├── mock_agent_demo.py    # CLI demo for cache & agent
├── app.py                # Streamlit web application
├── requirements.txt      # Python dependencies
├── .env.example          # Example for OpenAI API Key
└── test_*.py             # Older, individual test scripts (functionality now better tested in agent/cache __main__ blocks)
```

## 5. Setup and Usage

### Prerequisites
*   Python 3.10+
*   An OpenAI API Key

### Installation
1.  **Clone the repository:**
    ```bash
    git clone https://github.com/JoshW-dev/ai-agent-memory-cache.git
    cd ai-agent-memory-cache
    ```
2.  **Create and activate a virtual environment** (recommended):
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    # On Windows: venv\Scripts\activate
    ```
3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
4.  **Set up OpenAI API Key:**
    *   Rename `.env.example` to `.env`.
    *   Open `.env` and add your API key: `OPENAI_API_KEY="your_openai_api_key_here"`

### Running Demos and Tests

*   **Test `CapturingAgent` Logic (Recommended for understanding agent behavior):**
    ```bash
    python3 -m llm_module.capturing_agent
    ```
    *   **What to look for in the output:**
        *   Tool selection based on similarity scores (compare to `TOOL_SIMILARITY_THRESHOLD` of 0.4).
        *   Dynamic creation of new tools if no existing tool is suitable (e.g., creating `SetPlayerHealth` for "Set player's health to 75").
        *   Prevention of duplicate tool creation if the LLM suggests an existing name.
        *   Impact of simulated upvotes (identical prompts will show ~1.0 similarity to the upvoted tool).
        *   Impact of semantic similarity (e.g., "Set player speed to 20" might match with `SetPlayerHealth` if semantically similar).
        *   Impact of simulated downvotes (the downvoted tool will be excluded on retry, leading to a different tool choice, another new tool attempt, or a direct answer).
*   **Test `MemoryCache` Logic:**
    ```bash
    python3 -m memory_cache
    ```
*   **Run Interactive CLI Demo (integrates Agent and Cache):**
```bash
python3 mock_agent_demo.py
```
*   **Run Streamlit Web Application:**
    ```bash
    streamlit run app.py
    ```

## 6. Glossary

*   **`ActionSequence`**: `List[Dict[str, str]]`. A log of agent actions. Each dictionary details a step: tool used (or "ToolDefinitionAgent"/"DirectAnswer"), input, observation, similarity score (if a tool was picked by similarity), and the original user prompt for that step.
*   **`MemoryCache`**: Manages storage and retrieval of complete `ActionSequence`s, using prompt similarity and a reward score for self-healing of *cached sequences*.
*   **`CapturingAgent`**: The AI agent. Selects tools, creates new ones, generates inputs, and learns which tools are best for which prompts via feedback on *tool choice*.
*   **`BaseTool`**: Base class for tools. Stores `name`, `description`, its `primary_embedding`, and `additional_prompt_embeddings` learned from upvotes.
*   **`Primary Embedding`**: Semantic representation of a tool from its name and description.
*   **`Additional Representative Embeddings`**: Embeddings of user prompts for which a tool was upvoted, enhancing its semantic matching for similar future prompts.
*   **ChromaDB**: Vector database for the `MemoryCache`.
*   **Embeddings**: Numerical vectors representing text meaning.
*   **`SIMILARITY_THRESHOLD_TAU` (τ)**: For `MemoryCache` lookup. Min. similarity for a cached sequence to be a hit.
*   **`TOOL_SIMILARITY_THRESHOLD`**: For `CapturingAgent` tool selection. Min. similarity for choosing an existing tool before trying to create a new one.
*   **`SCORE_THRESHOLD_EPSILON` (ε)**: For `MemoryCache`. Cached sequences with scores below this are evicted.
*   **`REWARD_ALPHA` (α)**: EMA factor for `MemoryCache` sequence score updates.

## 7. Core Algorithms Overview

1.  **`CapturingAgent` Tool Selection & Execution:**
    *   Compares user prompt embedding to all tools (primary + additional embeddings for each tool).
    *   If best match ≥ `TOOL_SIMILARITY_THRESHOLD`, selects it.
    *   Else, attempts dynamic tool creation (LLM defines name/desc, checks for uniqueness, creates if valid).
    *   If a tool is chosen, LLM generates its input string.
    *   Tool is executed, or fallback to direct LLM answer.
2.  **`CapturingAgent` Feedback Processing (Tool Choice):**
    *   **Upvote:** Adds prompt embedding to the chosen tool's `additional_prompt_embeddings`.
    *   **Downvote:** Leads to exclusion of the tool for an immediate retry (handled by the caller, e.g., `app.py`).
3.  **`MemoryCache` Lookup (Action Sequences):**
    *   Queries ChromaDB for prompt embeddings similar to the user's prompt.
    *   Returns a stored `ActionSequence` if similarity ≥ `SIMILARITY_THRESHOLD_TAU` and score ≥ `SCORE_THRESHOLD_EPSILON`.
4.  **`MemoryCache` Reward Update (Action Sequences):**
    *   Adjusts the `score` of a cached `ActionSequence` using EMA based on user feedback.
    *   Evicts sequence if its score drops below `SCORE_THRESHOLD_EPSILON`.

## 8. Configuration Highlights

*   **`memory_cache.py` constants:** `OPENAI_EMBEDDING_MODEL`, `SIMILARITY_THRESHOLD_TAU`, `SCORE_THRESHOLD_EPSILON`, `REWARD_ALPHA`, `TOP_K_RESULTS`.
*   **`llm_module/capturing_agent.py` constants:** `TOOL_SIMILARITY_THRESHOLD`, `OPENAI_EMBEDDING_MODEL_FOR_TOOLS`, and various `...PROMPT_TEMPLATE`s.
*   **Tool descriptions in `llm_module/custom_tools.py`** are crucial for initial semantic matching.

## 9. Future Extensions

*   Refine UI for more nuanced feedback.
*   Explore more sophisticated downvote handling within the agent itself (e.g., temporarily lowering a tool's preference without full exclusion).